{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Model Classification for different authors' books\n",
    "\n",
    "## This section of the project will use the LSTM Neural Network to classify the authors using their books. \n",
    "\n",
    "### First, import the necessary packages to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "from keras.constraints import max_norm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "### Three authors of the books are: Jonathan Swift, Jane Austen, and Mary Shelley. The three popular books are collected for each of the authors, so there are total of 9 books in the dataset. For the ease of model fitting, each input unit will be the each paragraph of the book. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the function of generating dataframe for each of the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import text file and create dataframe\n",
    "def Swift(textfile):\n",
    "    # textfile has been edited so that it contains only body of the text\n",
    "    with open(textfile) as f:\n",
    "        lines = f.read()\n",
    "    book = lines.split(\"\\n\\n\") #split by paragraph\n",
    "    text = pd.Series(book, index = range(len(book)))\n",
    "    author = pd.Series(['Jonathan_Swift'] * len(book), index = range(len(book)))\n",
    "    df = pd.DataFrame({'author':author,\n",
    "                    'text':text})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Austen(textfile):\n",
    "    # textfile has been edited so that it contains only body of the text\n",
    "    with open(textfile) as f:\n",
    "        lines = f.read()\n",
    "    book = lines.split(\"\\n\\n\") #split by paragraph\n",
    "    text = pd.Series(book, index = range(len(book)))\n",
    "    author = pd.Series(['Jane_Austen'] * len(book), index = range(len(book)))\n",
    "    df = pd.DataFrame({'author':author,\n",
    "                    'text':text})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shelley(textfile):\n",
    "    # textfile has been edited so that it contains only body of the text\n",
    "    with open(textfile) as f:\n",
    "        lines = f.read()\n",
    "    book = lines.split(\"\\n\\n\") #split by paragraph\n",
    "    text = pd.Series(book, index = range(len(book)))\n",
    "    author = pd.Series(['Mary_Shelley'] * len(book), index = range(len(book)))\n",
    "    df = pd.DataFrame({'author':author,\n",
    "                    'text':text})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "swift_1 = Swift('Swift_1.txt')\n",
    "swift_2 = Swift('Swift_2.txt')\n",
    "swift_3 = Swift('Swift_3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "austen_1 = Austen('Austen_1.txt')\n",
    "austen_2 = Austen('Austen_2.txt')\n",
    "austen_3 = Austen('Austen_3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "shelley_1 = Shelley('Shelley_1.txt')\n",
    "shelley_2 = Shelley('Shelley_2.txt')\n",
    "shelley_3 = Shelley('Shelley_3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, concatenate all the data for different authors into one large dataframe. There are total of 10818 rows, which means there are 10818 paragraphs in total. The example of the dataframe is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([swift_1,swift_2,swift_3,austen_1,austen_2,austen_3,shelley_1,shelley_2,shelley_3],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10818, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jonathan_Swift</td>\n",
       "      <td>It is a melancholy object to those, who walk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jonathan_Swift</td>\n",
       "      <td>I think it is agreed by all parties, that this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jonathan_Swift</td>\n",
       "      <td>But my intention is very far from being confin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jonathan_Swift</td>\n",
       "      <td>As to my own part, having turned my thoughts f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jonathan_Swift</td>\n",
       "      <td>There is likewise another great advantage in m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author                                               text\n",
       "0  Jonathan_Swift  It is a melancholy object to those, who walk t...\n",
       "1  Jonathan_Swift  I think it is agreed by all parties, that this...\n",
       "2  Jonathan_Swift  But my intention is very far from being confin...\n",
       "3  Jonathan_Swift  As to my own part, having turned my thoughts f...\n",
       "4  Jonathan_Swift  There is likewise another great advantage in m..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "### After the text of the data has been stored, it needs to be processed. Below function will replace all the punctuations, symbols and also take out the stopwords. The text will all be transformed into lower case and there will be no digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)\n",
    "    text = text.replace('x', '')\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['btext'].apply(clean_text)\n",
    "df['text'] = df['text'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, the below 'text' is the cleaned version of the dataframe. We can compare this dataframe with above original text. There are only useful words contain in this cleaned dataframe. Next these text will be tokenized and use word embedding for the model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jonathan_Swift</td>\n",
       "      <td>melancholy object walk great town travel count...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jonathan_Swift</td>\n",
       "      <td>think agreed parties prodigious number ofchild...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jonathan_Swift</td>\n",
       "      <td>intention far confined provide thechildren pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jonathan_Swift</td>\n",
       "      <td>part turned thoughts many years upon thisimpor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jonathan_Swift</td>\n",
       "      <td>likewise another great advantage scheme willpr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author                                               text\n",
       "0  Jonathan_Swift  melancholy object walk great town travel count...\n",
       "1  Jonathan_Swift  think agreed parties prodigious number ofchild...\n",
       "2  Jonathan_Swift  intention far confined provide thechildren pro...\n",
       "3  Jonathan_Swift  part turned thoughts many years upon thisimpor...\n",
       "4  Jonathan_Swift  likewise another great advantage scheme willpr..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The maximum number of words to be used is set as 5000, and it is the most frequent words showing in the data. Max number of words in each complaint is set as 500, since one paragraph will not be too long. The embedding dimension is set to be 100. \n",
    "\n",
    "### 'Tokenizer' method will split text into words are generate word vectors. The number of unique words that are in these books are 67501."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 67501 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 5000\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since there are too many words, input vector needs to be padded into the maximum sequence where I set to be 500. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (10818, 500)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(df['text'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Y vector are now the names of the authors. It needs to be converted into vectors as well. 'get_dummies' function will automatically generate vector for those authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (10818, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = pd.get_dummies(df['author']).values\n",
    "print('Shape of label tensor:', Y.shape)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, the dataset is ready to split into training and testing sets. The size of the training set is 90% of the total dataset, and the rest of the dataset is used for testing set. The 'random_state' will set seed to these testing and training set so that everytime running this code will have the same training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9736, 500) (9736, 3)\n",
      "(1082, 500) (1082, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Baseline model\n",
    "### Below is the baseline for the LSTM Neural Network model. The activation function is 'tanh' for the LSTM Network. The baseline model has added the dropout of 0.5 since LSTM Network can easily get into overfitting model. The dense unit would be 3 at the end since we have three different authors that will be classified. The activation function of Dense function is a 'softmax' function. It will be complied into the 'categorical crossentropy' since there are three categories, which are three authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 8762 samples, validate on 974 samples\n",
      "Epoch 1/10\n",
      "8762/8762 [==============================] - 27s 3ms/step - loss: 0.8667 - accuracy: 0.6303 - val_loss: 0.5422 - val_accuracy: 0.8101\n",
      "Epoch 2/10\n",
      "8762/8762 [==============================] - 26s 3ms/step - loss: 0.4258 - accuracy: 0.8341 - val_loss: 0.3117 - val_accuracy: 0.8522\n",
      "Epoch 3/10\n",
      "8762/8762 [==============================] - 25s 3ms/step - loss: 0.2249 - accuracy: 0.9217 - val_loss: 0.2217 - val_accuracy: 0.9168\n",
      "Epoch 4/10\n",
      "8762/8762 [==============================] - 24s 3ms/step - loss: 0.1511 - accuracy: 0.9466 - val_loss: 0.2195 - val_accuracy: 0.9189\n",
      "Epoch 5/10\n",
      "8762/8762 [==============================] - 25s 3ms/step - loss: 0.1241 - accuracy: 0.9556 - val_loss: 0.2308 - val_accuracy: 0.9086\n",
      "Epoch 6/10\n",
      "8762/8762 [==============================] - 25s 3ms/step - loss: 0.1090 - accuracy: 0.9569 - val_loss: 0.2470 - val_accuracy: 0.9138\n",
      "Epoch 7/10\n",
      "8762/8762 [==============================] - 24s 3ms/step - loss: 0.1235 - accuracy: 0.9546 - val_loss: 0.2378 - val_accuracy: 0.9148\n",
      "Epoch 8/10\n",
      "8762/8762 [==============================] - 24s 3ms/step - loss: 0.0995 - accuracy: 0.9591 - val_loss: 0.2263 - val_accuracy: 0.9127\n",
      "Epoch 9/10\n",
      "8762/8762 [==============================] - 24s 3ms/step - loss: 0.0918 - accuracy: 0.9607 - val_loss: 0.2536 - val_accuracy: 0.9086\n",
      "Epoch 10/10\n",
      "8762/8762 [==============================] - 25s 3ms/step - loss: 0.0897 - accuracy: 0.9618 - val_loss: 0.2529 - val_accuracy: 0.9148\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(32, activation = 'tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082/1082 [==============================] - 1s 1ms/step\n",
      "Test set\n",
      "  Loss: 0.272\n",
      "  Accuracy: 0.918\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 100)          500000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 517,123\n",
      "Trainable params: 517,123\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the above result, the model is overfitting according to the accuracy of validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Neural Network model\n",
    "### Here, the model constraints has been added in order to fix the overfitting. Instead of using the dropout method, this model used to specify the maximum number of the norm of kernel vector, reccurent vector, and bias vector. I used 3 as the maximun number of norm of the kernel vector, norm of reccurent vector, and norm of bias vector. If the number of norm of the vector exceed 3, then it will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8762 samples, validate on 974 samples\n",
      "Epoch 1/6\n",
      "8762/8762 [==============================] - 27s 3ms/step - loss: 0.8405 - accuracy: 0.6258 - val_loss: 0.5465 - val_accuracy: 0.7721\n",
      "Epoch 2/6\n",
      "8762/8762 [==============================] - 26s 3ms/step - loss: 0.4326 - accuracy: 0.8350 - val_loss: 0.3554 - val_accuracy: 0.8470\n",
      "Epoch 3/6\n",
      "8762/8762 [==============================] - 26s 3ms/step - loss: 0.2587 - accuracy: 0.8797 - val_loss: 0.2572 - val_accuracy: 0.8881\n",
      "Epoch 4/6\n",
      "8762/8762 [==============================] - 25s 3ms/step - loss: 0.1535 - accuracy: 0.9482 - val_loss: 0.2109 - val_accuracy: 0.9168\n",
      "Epoch 5/6\n",
      "8762/8762 [==============================] - 25s 3ms/step - loss: 0.1079 - accuracy: 0.9563 - val_loss: 0.1956 - val_accuracy: 0.9230\n",
      "Epoch 6/6\n",
      "8762/8762 [==============================] - 29s 3ms/step - loss: 0.0913 - accuracy: 0.9618 - val_loss: 0.2005 - val_accuracy: 0.9230\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(32, kernel_constraint=max_norm(3), recurrent_constraint=max_norm(3), \n",
    "               bias_constraint=max_norm(3)))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 6\n",
    "batch_size = 128\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082/1082 [==============================] - 2s 1ms/step\n",
      "Test set\n",
      "  Loss: 0.220\n",
      "  Accuracy: 0.915\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 100)          500000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 517,123\n",
      "Trainable params: 517,123\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, above model has the good performance without any overfitting problem. Test accuracy is 0.915, where training accuracy is 0.9618. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "### After the model, look at the confusion matrix of the model to see the detailed prediction accuracy for each of the output vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/confusion_matrix.py:193: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "[[615   1  47]\n",
      " [  7  78  16]\n",
      " [ 17   4 297]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict_classes(X_test)\n",
    "sum(Y_pred == Y_test.argmax(axis = 1))\n",
    "con_mat = tf.confusion_matrix(labels=Y_test.argmax(axis = 1), predictions=Y_pred)\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "        print(sess.run(con_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above confusion matrix is in the order of Jonathan Swift, Jane Austen, and Mary Shelley.  For Jane Austen, the number of paragraphs are a lot shorter than other two authors', so that there are only 5 paragraphs has been misclassified. Jonathan Swift's paragraphs are classified more accurate than Mary Shelley's. Overall, the performance is good for all of the three author's paragraphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resources for LSTM model: https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
